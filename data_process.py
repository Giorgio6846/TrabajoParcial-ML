# -*- coding: utf-8 -*-
"""data_process.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w41HAXvNWx__sMUhPY18lN7-mvxL00XM

# Importe de librerias
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.naive_bayes import GaussianNB
from imblearn.under_sampling import NearMiss
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
scaler = MinMaxScaler()

"""# Carga de datos"""

df = pd.read_csv("../data/credit_card_transactions.csv")
df.head()

"""# EDA"""

df.head()

df.dtypes

"""## Transformacion de datos"""

df['Time'] = pd.to_datetime(df['Time'], errors = 'coerce')
df['MCC'] = df['MCC'].astype(object)
df['Weekday'] = df['Time'].dt.weekday
df['Hour'] = df['Time'].dt.hour
df['Month'] = df['Time'].dt.month

df.head()

df.describe()

df = df.drop(['CardholderName','TransactionID', 'Time', 'IPAddress'], axis=1)
df.isna().sum()

plt.boxplot(df['Amount'])

plt.boxplot(df['TransactionSpeed'])

# IQR Amount
q1 = df['Amount'].quantile(0.25)
q3 = df['Amount'].quantile(0.75)
iqr = q3 - q1

limite_superior = q3 + 1.5 * iqr

df['is_large_amount'] = (df['Amount'] > limite_superior).astype(int)


# IQR TransactionSpeed
q1 = df['TransactionSpeed'].quantile(0.25)
q3 = df['TransactionSpeed'].quantile(0.75)
iqr = q3 - q1

limite_superior = q3 + 1.5 * iqr

df['is_slow_transaction'] = (df['TransactionSpeed'] > limite_superior).astype(int)

#Data desbalanceada
fraudes = (df['Fraud'] == 0).sum()
noFraudes = (df['Fraud'] == 1).sum()

print('Proporciones')
print(f'Fraudes: {noFraudes/ (fraudes+noFraudes)}')
print(f'No fraudes: {fraudes/ (fraudes+noFraudes)}')

"""# SPLIT"""

X = df.drop('Fraud', axis=1)
y = df['Fraud']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

"""### Normalizacion de datos"""

#Inicializacion de normalizador

categoricFeatures = ['MerchantName','Location', 'MCC', 'Device']
numericFeatures = ['Amount', 'TransactionSpeed']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', MinMaxScaler(), numericFeatures),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categoricFeatures)
    ]
)

X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

#Balanceo con NearMiss3

nm = NearMiss(version=3, n_neighbors=3,n_neighbors_ver3=3 )
X_train_balanced, y_train_balanced = nm.fit_resample(X_train_processed, y_train)
print(pd.Series(y_train_balanced).value_counts())

pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', GaussianNB())
])

model = GaussianNB()
model.fit(X_train_balanced, y_train_balanced)

y_pred = model.predict(X_test_processed)

print(classification_report(y_test, y_pred))

df.to_csv("../data/process_data.csv")